# RAG System Project

A robust Retrieval-Augmented Generation (RAG) system built with FastAPI that enables document upload, intelligent processing, and AI-powered retrieval. Upload files, automatically process them into searchable chunks stored in MongoDB, and retrieve contextual information for your AI applications.

## ğŸ—ï¸ Architecture Overview

### Core Components

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI API   â”‚â”€â”€â”€â”€â–¶â”‚   Controllers   â”‚
â”‚  (Upload/Query) â”‚     â”‚   Routes        â”‚     â”‚  (Business      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   Logic)        â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                                                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MongoDB       â”‚â—€â”€â”€â”€â”€â”‚   Data Models   â”‚â—€â”€â”€â”€â”€â”‚   File Storage  â”‚
â”‚  (Chunks &      â”‚     â”‚  (Pydantic)     â”‚     â”‚  (Project-based â”‚
â”‚   Projects)     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   Organization) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â–²
                                                          â”‚
                                                          â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   LangChain     â”‚â”€â”€â”€â”€â–¶â”‚   Document      â”‚
                        â”‚  Text Splitter  â”‚     â”‚   Loaders       â”‚
                        â”‚  (Chunking)     â”‚     â”‚  (PDF, TXT)     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Document Upload** â†’ File validation â†’ Unique naming â†’ Project storage
2. **Document Processing** â†’ Content extraction â†’ Text chunking â†’ Metadata preservation  
3. **Data Storage** â†’ MongoDB chunks â†’ Project organization â†’ Retrieval indexing

## ğŸ› ï¸ Technical Stack

- **Backend Framework**: FastAPI with async/await patterns
- **Database**: MongoDB with Motor (async Python driver)
- **Document Processing**: LangChain (text splitting, document loading)
- **PDF Processing**: PyMuPDF (FitzPDF) for efficient PDF extraction
- **Data Validation**: Pydantic v2 with custom validators
- **File Handling**: aiofiles for async I/O operations
- **Containerization**: Docker & Docker Compose
- **Python Version**: 3.12+
- **Additional Libraries**: pymongo, aiofiles, python-dotenv, python-multipart

## ğŸ“ Project Structure

```text
src/
â”œâ”€â”€ main.py                          # FastAPI application & lifespan context
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ helper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py                    # Application settings management
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                      # Health/version endpoints
â”‚   â”œâ”€â”€ data_route.py                # File upload & processing endpoints
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ dataproces_schemas.py    # Request/response schemas
â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ BaseContoller.py             # Base controller functionality
â”‚   â”œâ”€â”€ DataController.py            # File validation & storage
â”‚   â”œâ”€â”€ ProcessController.py         # Document processing & chunking
â”‚   â””â”€â”€ __pycache__/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ BaseDataModel.py             # Base async MongoDB model
â”‚   â”œâ”€â”€ ChunkModel.py                # Chunks collection DAL (async)
â”‚   â”œâ”€â”€ ProjectModel.py              # Projects collection DAL (async)
â”‚   â”œâ”€â”€ AssetModel.py                # Assets collection DAL (async)
â”‚   â”œâ”€â”€ db_schemas/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chunks_schemas.py        # ChunkSchema with indexes
â”‚   â”‚   â”œâ”€â”€ project_shemas.py        # ProjectSchema with indexes
â”‚   â”‚   â”œâ”€â”€ asset.py                 # AssetSchema with indexes
â”‚   â”‚   â””â”€â”€ __pycache__/
â”‚   â”œâ”€â”€ enums/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ProcesseEnums.py         # Document type enums
â”‚   â”‚   â”œâ”€â”€ ResponseEnums.py         # API response enums
â”‚   â”‚   â””â”€â”€ __pycache__/
â”‚   â””â”€â”€ __pycache__/
â””â”€â”€ assets/
    â””â”€â”€ files/                       # File storage (organized by project)
        â””â”€â”€ {project_id}/            # Project-specific directories

docker/
â”œâ”€â”€ docker-compose.yml               # MongoDB service definition
â”œâ”€â”€ .env.example                     # Environment template
â”œâ”€â”€ .gitignore                       # Docker-specific gitignore
â””â”€â”€ mongo-data/                      # MongoDB persistent storage

.gitignore                          # Root gitignore
README.md                           # This file
LICENSE                            # Project license
```

## ğŸš€ API Endpoints

### Base Endpoints

- `GET /api/v1/` - Application information and health check

### Data Management Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/data/upload/{project_id}` | Upload files to a project (returns asset_id) |
| `POST` | `/api/v1/data/processall/{project_id}` | Process all files in project, save chunks to MongoDB |
| `POST` | `/api/v1/data/processone/{project_id}` | Process single file, save chunks with optional reset |

### Response Structure

All endpoints return JSON responses with status indicators:

```json
{
  "status": "success_code",
  "message": "Descriptive message",
  "data": {}
}
```

### Request/Response Examples

**Upload File:**

```bash
curl -X POST "http://localhost:8000/api/v1/data/upload/my_project" \
     -F "file=@document.pdf"

# Response:
{
  "status": "file_upload_success",
  "file_path": "/path/to/file",
  "file_id": "unique_filename",
  "asset_id": "507f1f77bcf86cd799439011"
}
```

**Process Single File:**

```bash
curl -X POST "http://localhost:8000/api/v1/data/processone/my_project" \
     -H "Content-Type: application/json" \
     -d '{
       "file_id": "abc123_document.pdf",
       "chunk_size": 1000,
       "overlap_size": 100,
       "do_reset": false
     }'

# Response:
{
  "status": "processing_success",
  "total_chunks": 42,
  "inserted_chunks": 42,
  "chunks": [...]
}
```

**Process All Files:**

```bash
curl -X POST "http://localhost:8000/api/v1/data/processall/my_project"

# Response:
{
  "status": "processing_success",
  "total_files": 5,
  "processed_files": 5,
  "failed_files": 0,
  "total_chunks": 150,
  "inserted_chunks": 150
}
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in the `src/` directory with the following variables:

```bash
# Application Configuration
APP_NAME=RAG-System
APP_VERSION=1.0.0

# File Upload Configuration
FILE_ALLOWED_TYPES=["application/pdf", "text/plain"]
FILE_MAX_SIZE=50                    # Maximum file size in MB
CHUNK_SIZE=1000                     # Default chunk size in characters
CHUNK_OVERLAP=100                   # Default overlap between chunks

# Database Configuration
MONGO_URI=mongodb://root:example@localhost:27017
MONGO_DB_NAME=rag_system_db
```

### Docker Environment (.env in docker/)

```bash
# MongoDB Credentials
MONGO_INITDB_ROOT_USERNAME=root
MONGO_INITDB_ROOT_PASSWORD=example
```

âš ï¸ **Security Note**: Change default MongoDB credentials in production!

## ğŸ“‹ Prerequisites & Installation

### Prerequisites

- Python 3.12+
- Docker & Docker Compose
- Git

### Quick Start

1. **Clone the repository:**
   ```bash
   git clone https://github.com/SaeedNeamtallah/Rag-System-Project.git
   cd Rag-System-Project
   ```

2. **Create and activate virtual environment:**
   ```bash
   # On Linux/Mac
   python3 -m venv venv
   source venv/bin/activate

   # On Windows
   python -m venv venv
   venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   cd src
   pip install -r requirements.txt
   ```

4. **Start MongoDB with Docker Compose:**
   ```bash
   cd ../docker
   docker-compose up -d
   ```

5. **Create `.env` file in src/:**
   ```bash
   cd ../src
   # Edit the environment variables as shown in Configuration section
   ```

6. **Run the application:**
   ```bash
   uvicorn main:app --reload --host 0.0.0.0 --port 8000
   ```

7. **Access the API:**
   - API Documentation: `http://localhost:8000/docs`
   - ReDoc Documentation: `http://localhost:8000/redoc`
   - API Base URL: `http://localhost:8000/api/v1`

## ğŸ“Š Database Schema

### Collections

#### `projects` Collection
```json
{
  "_id": ObjectId,
  "project_id": "string (unique)",
  "created_at": "ISO datetime",
  "updated_at": "ISO datetime"
}
```

#### `chunks` Collection
```json
{
  "_id": ObjectId,
  "chunk_text": "string",
  "chunk_metadata": "object",
  "chunk_order": "integer (â‰¥ 1)",
  "chunk_project_id": "ObjectId (ref: projects._id)",
  "created_at": "ISO datetime",
  "updated_at": "ISO datetime"
}
```

#### `assets` Collection
```json
{
  "_id": ObjectId,
  "asset_project_id": "ObjectId (ref: projects._id)",
  "asset_type": "string (e.g., 'file')",
  "asset_name": "string (filename)",
  "asset_size": "integer (bytes)",
  "asset_config": "object (optional)",
  "asset_pushed_at": "ISO datetime"
}
```

### Indexes

- `projects`: Unique index on `project_id`
- `chunks`: Index on `chunk_project_id`
- `assets`: Composite unique index on (`asset_project_id`, `asset_name`)

## ğŸ› Recent Fixes & Improvements

### v1.0.0 Updates
- âœ… Fixed data persistence: chunks and projects now properly saved to MongoDB
- âœ… Implemented async factory pattern for all models (ChunkModel, ProjectModel, AssetModel)
- âœ… Added comprehensive error handling in all endpoints
- âœ… Implemented proper MongoDB schema validation with Pydantic
- âœ… Added automatic index creation for all collections
- âœ… Fixed asset tracking with dedicated AssetModel
- âœ… Improved file upload with validation and error responses
- âœ… Added project lookup before processing with 404 handling
- âœ… Implemented batch chunk insertion for performance

## ğŸ§ª Testing

### Manual Testing

```bash
# 1. Upload a PDF file
curl -X POST "http://localhost:8000/api/v1/data/upload/test_project" \
     -F "file=@sample.pdf"

# 2. Process the uploaded file
curl -X POST "http://localhost:8000/api/v1/data/processone/test_project" \
     -H "Content-Type: application/json" \
     -d '{
       "file_id": "abc123_sample.pdf",
       "chunk_size": 1000,
       "overlap_size": 100,
       "do_reset": false
     }'

# 3. Verify chunks in MongoDB
# Connect to MongoDB and check: db.chunks.find({"chunk_project_id": ObjectId("...")})
```

## ğŸš€ Deployment

### Production Considerations

1. **Environment Variables**: Update all `.env` files with production values
2. **MongoDB**: Use MongoDB Atlas or managed service with authentication
3. **Security**: Enable HTTPS, add CORS policies, implement rate limiting
4. **Logging**: Configure centralized logging for production monitoring
5. **Docker**: Build optimized production images with multi-stage builds

## ğŸ“ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Contributors

- Saeed Neamtallah (@SaeedNeamtallah)
